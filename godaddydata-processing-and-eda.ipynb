{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Tracking useful datasets\n\n1. Start\n    1. train_base\n    1. census_data\n2. Preprocessing\n    1. train_base\n    1. thin_census_data (converted form of census for join)\n    ","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Installation of libraries as required\n\n!pip install sklearn","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Importing libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Define custom functions or classes","metadata":{}},{"cell_type":"code","source":"# None required right now","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Processing data\n\n1. Import the dataset\n2. Generate high level summaries and pair plots","metadata":{}},{"cell_type":"markdown","source":"## Import data","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"train_base = pd.read_csv('/kaggle/input/godaddy-microbusiness-density-forecasting/train.csv')\ntest_base = pd.read_csv('/kaggle/input/godaddy-microbusiness-density-forecasting/test.csv')\n\ncensus_data = pd.read_csv('/kaggle/input/godaddy-microbusiness-density-forecasting/census_starter.csv')\nsample_submission = pd.read_csv('/kaggle/input/godaddy-microbusiness-density-forecasting/sample_submission.csv')\n\ntrain_base.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"census_data.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head(5)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Notes:\n\n1. Additional census attributes need to be mapped through cfips attribute\n2. Census data needs to be transformed into a thin format for usablity and joins\n3. Census data is yearly, whereas the microbusiness data is monthly. We will need to keep this in ming when modelling later\n4. Final submission needs a key based on cfips and date combination. Thus we will be working with 2 sets of keys to join and model data.\n    1. cfips + full date\n    2. cfips + year (for joining census data)","metadata":{}},{"cell_type":"markdown","source":"## Basic summary\nWe will try and understand the type of data we will work with","metadata":{}},{"cell_type":"code","source":"train_base.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base.isnull().any()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_base.dtypes","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"census_data.isnull().sum(axis = 0)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Pre-processing\n\n1. Convert census data into thin format\n1. Fill in the missing data in census data\n1. Add relevant keys to census and train data\n1. Validate if all keys are present in both datasets (helps determine potential upcoming missing values in data)\n\nWe may need to perform additional processing based on the validation results","metadata":{}},{"cell_type":"markdown","source":"### Transform census data","metadata":{}},{"cell_type":"code","source":"census_data_cols = census_data.columns.values\n\n# We will use \"_\" to seperate each string and create a new thin dataframe\n# This dataframe will be at a \"cfips\" and \"year\" level to enable join with the base dataset\n\ntemp = pd.DataFrame(census_data_cols, index= None)\n\ntemp.rename(columns = {0:\"col_name\"}, inplace = True)\n\n# Remove cfips to make life easy\n\ntemp = temp.loc[temp['col_name'] != \"cfips\",:]\n\ntemp['year'] = temp['col_name'].str.slice(-4).astype('int')\ntemp['temp'] = \"_\"+temp['year'].astype('str')\n\n#temp['new_col_name'] = temp['col_name'].str.replace(temp['temp'], \"\")\n# Since expected method does not work, we will remove the last 5 characters for ease\n\ntemp['new_col_name'] = temp['col_name'].str.slice(start = 0, stop = -5).astype('str')\n\ntemp.drop(columns = (\"temp\"), inplace = True)\ntemp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Convert census to thin format\n\nWe will run a loop to iterate through our gegerated columns, and append data to empty dataframe\nWe will summarize the dataframe to make format consistent without loss of data\nHopefully there is a better way to do this","metadata":{}},{"cell_type":"code","source":"new_cols = list(temp['new_col_name'].unique())\n\n# Add the year and cfips columns \nnew_cols.insert(0, (\"year\"))\nnew_cols.insert(0, (\"cfips\"))\n\n# Make empty dataframe\nthin_census_data = pd.DataFrame(columns = new_cols)\n\n# Generate all possible combinations of cfips and year from just census data\n\nfor row in temp.itertuples():\n    col_name = row.col_name\n    year = row.year\n    new_col_name = row.new_col_name\n    \n    # Make data to append/join\n    temp_data = census_data.loc[:,(\"cfips\",col_name)]# Get the required column along with IDs\n    temp_data[\"year\"] = year # Add year column\n    \n    temp_data = temp_data.loc[:,(\"cfips\",\"year\",col_name)] # Reorder columns for consistency\n    temp_data.rename(columns = {col_name:new_col_name}, inplace = True)\n    \n    # Append data to out main dataframe\n    thin_census_data = pd.concat((thin_census_data,temp_data), axis = 0)\n\n\n# Summarize the dataset to get the final \nthin_census_data = thin_census_data.groupby(by = [\"cfips\",\"year\"], as_index = False).first().reset_index()\nthin_census_data.drop(columns = 'index', inplace = True)\n\nthin_census_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Validate the transformed dataset\n\nWe will check if everything went smoothly using simple summaries across original and converted dataset","metadata":{}},{"cell_type":"code","source":"# Original summary\ncensus_data.describe()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"thin_census_data.groupby(by = ['year']).min()\nthin_census_data.groupby(by = ['year']).max()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Process the train dataset\n\nWe will extract relevant information from the dataset and join the census data to make the final dataset\n\n1. Extract the year and month out of the date columns\n2. Reorder columns for ease of reading","metadata":{}},{"cell_type":"code","source":"train_base.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge train and test datasets\n\nSimplifies processing and appending required data ","metadata":{}},{"cell_type":"code","source":"train_base['is_test'] = 0\ntest_base['is_test'] = 1\n\nall_data = train_base.append(test_base)\nall_data.reset_index(drop=True, inplace=True)\n\nall_data.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Add required columns and tweak data","metadata":{}},{"cell_type":"code","source":"# Extract the year from dates\nall_data['first_day_of_month'] = pd.to_datetime(all_data['first_day_of_month'])\n\n# Sort data to ensure proper imputations of data\nall_data.sort_values(by = ['cfips','first_day_of_month'], inplace = True, ignore_index = True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Extracting important numbers from date\nall_data['year'] = all_data['first_day_of_month'].dt.year\nall_data['month'] = all_data['first_day_of_month'].dt.month\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Add the state and county names where missing\n\nall_data['county'] = all_data.groupby('cfips')['county'].ffill()\nall_data['state'] = all_data.groupby('cfips')['state'].ffill()\n\nall_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign numerical ids to each entry within cfip\n\n# Number of densities for each county, will assign incremental number for each succeeding date\nall_data[\"time_step\"] = all_data.groupby(['cfips'])['first_day_of_month'].cumcount()\n\n# Use factorize to assign numerical values. The final [0] is to pick the numbers assigned and ignore the index with text\nall_data['county_num'] = (all_data['county'] + all_data['state']).factorize()[0]\n# Same as above for states\nall_data['state_num'] = all_data['state'].factorize()[0]\n\nall_data.tail()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lag = 1\n\n# Shift moves a past time step to 1 unit forward. Thus we do bfill since the first date will have blank values\nall_data[f'mbd_lag_{lag}'] = all_data.groupby('cfips')['microbusiness_density'].shift(lag).bfill()\n\n\nall_data[f'mbd_lag_ratio_{lag}'] = (all_data['microbusiness_density'] / all_data[f'mbd_lag_{lag}']).fillna(1).clip(0, None) - 1\n\n\nall_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lag = 1\n\n# Shift moves a past time step to 1 unit forward. Thus we do bfill since the first date will have blank values\nall_data[f'mbd_lag_{lag}'] = all_data.groupby('cfips')['microbusiness_density'].shift(lag).bfill()\n\n########\n# Calculate a difference ratio for lag 1 MBD  \nall_data[f'mbd_lag_ratio_{lag}'] = (all_data['microbusiness_density'] / all_data[f'mbd_lag_{lag}']).fillna(1).clip(0, None) - 1\n# Reassign infinite ratios as 0\nall_data.loc[(all_data[f'mbd_lag_{lag}']==0), 'dif'] = 0\n# Reassign infinite ratios with recent uptake in MBD as 1.\nall_data.loc[(all_data[f'microbusiness_density']>0) & (all_data[f'mbd_lag_{lag}']==0), f'mbd_lag_ratio_{lag}'] = 1\n\n# convert to absolute value (not sure why)\nall_data[f'mbd_lag_ratio_{lag}'] = all_data[f'mbd_lag_ratio_{lag}'].abs()\n\n# Plot to check any inconsistencies\nall_data.groupby('time_step')[f'mbd_lag_ratio_{lag}'].sum().plot()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Simple reorder for more sensible data","metadata":{}},{"cell_type":"code","source":"# Reorder columns\nreorder_cols = ('row_id', 'cfips', 'year', 'month', 'first_day_of_month', 'county', 'state', 'active', 'microbusiness_density')\nall_data = all_data.loc[:,reorder_cols]\n\nall_data.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Cross-Validate both datasets\n\n1. We will check if we have match for all (or most) keys in dataset\n\nObservations:\n1. Census data has extra counties included\n1. Census data is not available for 2022 (which is logical).\n    1. THus we may need to generate some projecttions/estimates for 2022. We can either use same 2021 numbers or determine average growth rate based on bucketed counties\n","metadata":{}},{"cell_type":"code","source":"train_cfips = train_base['cfips'].unique()\ntrain_years = train_base['year'].unique()\n\ncensus_cfips = thin_census_data['cfips'].unique()\ncensus_years = thin_census_data['year'].unique()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check county IDs","metadata":{}},{"cell_type":"code","source":"print(\"Training data\")\nprint(train_cfips)\nprint(\"size = \",train_cfips.shape)\n\nprint(\"\\nCensus Data\")\nprint(census_cfips)\nprint(\"size = \",census_cfips.shape)\n\n# Print missing from each sets\nprint(\"Missing from census data (but are present in training data):\")\nprint(train_cfips[~np.in1d(train_cfips, census_cfips)])\n\nprint(\"Missing from training data (and are available in census):\")\nprint(census_cfips[~np.in1d(census_cfips, train_cfips)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Check years","metadata":{}},{"cell_type":"code","source":"print(\"Training data\")\nprint(train_years)\nprint(\"size = \",train_years.shape)\n\nprint(\"\\nCensus Data\")\nprint(census_years)\nprint(\"size = \",census_years.shape)\n\n# Print missing from each sets\nprint(\"Missing from census data (but are present in training data):\")\nprint(train_years[~np.in1d(train_years, census_years)])\n\nprint(\"Missing from training data (and are available in census):\")\nprint(census_years[~np.in1d(census_years, train_years)])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# EDA\n\nAs we have some relevant information on the data, we can perform EDA to establish a high level understanding before proceeding.","metadata":{}},{"cell_type":"markdown","source":"## EDA - Training data\n\nVisualize the timeline using line charts","metadata":{}},{"cell_type":"markdown","source":"### Microbusiness density over time\n\nSince we have few counties doing cnonsiderably better than others (again expected information), we may need to bucket counties into groups for easy managable visualizations","metadata":{}},{"cell_type":"code","source":"# Making a copy\neda_train_data = train_base","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick pair plot\nimport seaborn as sns\n\nsns.pairplot(data = eda_train_data, hue='year')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Bucket Counties based on micro density variable\n# We will have to tag outliers and then decile the data\n# Based on the pair plot, we will tag the entries with \n# We will use deciles for initial view\n# Keep in mind, decile ranking assigns first label to lowest decile\n\n# Summarize data to get avergae density for each county over years.\ntemp_avg_density = eda_train_data.groupby(['cfips']).agg({'microbusiness_density':'mean'}).reset_index()\n\n# Store list of cfips based on graphs\n# We will treat average above 30 as an outlier\noutlier_cfips = temp_avg_density.loc[temp_avg_density['microbusiness_density'] >= 30.0, ['cfips']]\noutlier_cfips.loc[:,'microdensity_buckets'] = \"outlier\"\n\n# We bucket the rest\nnormal_cfips = temp_avg_density.loc[~temp_avg_density['cfips'].isin(outlier_cfips['cfips'].unique()),['cfips','microbusiness_density']]\nnormal_cfips.loc[:,['microdensity_buckets']] = pd.qcut(temp_avg_density['microbusiness_density'],10, labels = False)\n\n# normal_cfips.head()\nnormal_cfips = normal_cfips.loc[:,[\"cfips\",\"microdensity_buckets\"]]\n\ncfip_density_buckets = pd.concat((normal_cfips, outlier_cfips), axis = 0).reset_index()\n# cfip_density_buckets\n\n\n# Plot for inspection\nsns.pairplot(data = temp_avg_density)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign the average micro business density buckets to data\neda_train_data = pd.merge(left = eda_train_data, right = cfip_density_buckets,on = 'cfips', how = 'left')\n\n# Trying to see how data ends up\n# Quick observations\n# Outlier buckets seem to have counties that started small (but had relatively higher density to start off with), but have high density over years\n# These counties may already have decent population\ntemp = eda_train_data.groupby(('microdensity_buckets'), as_index = False).agg(min_density = (\"microbusiness_density\",np.min),max_density = (\"microbusiness_density\",np.max))\ntemp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Through visual inspection, we need to classify certain counties as outliers to get a better idea of trends\n# Quick scatter plot to visualise the outliers\n\ng = sns.FacetGrid(data = eda_train_data,col = 'microdensity_buckets', col_wrap = 3)\n\ng.map(sns.lineplot, \"first_day_of_month\", \"microbusiness_density\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Active over time\n\nWe will check the active attribute and see if we can get some information from it.\nAlso, since we will not have this attribute available for test set, we will need to figure out a imputation for this variable.\n\nGood idea is to check coorelation with other attributes to make the task easy","metadata":{}},{"cell_type":"code","source":"temp_avg_active = eda_train_data.groupby(['cfips'], as_index=False).agg({'active':'mean'})\n\n# Store list of cfips based on graphs\n# Difficult to see actual values due to extreme outlier. Remove and recheck\n\n# We will do trial and error to evaluate the best cutoff based on average active businesses\n# Looking at granular plots, a good cutoff for active attribute outliers at 20k\noutlier_cfips_active = temp_avg_active.loc[temp_avg_active['active'] >= 2.0e4, ['cfips']]\n\noutlier_cfips_active['active_buckets'] = \"outlier\"\n# None here. We will merge it regardless for redundancy\nzero_cfips_active = temp_avg_active.loc[temp_avg_active['active'] == 0, 'cfips']\n\nif len(zero_cfips_active) != 0:\n    zero_cfips_active['active_buckets'] = \"zero\"\n    outlier_cfips_active = pd.concat((outlier_cfips_active, zero_cfips_active), axis = 0)\n\n# Generate buckets for the remaining counties\ntemp_avg_active = temp_avg_active.loc[~temp_avg_active['cfips'].isin(outlier_cfips_active['cfips']), :]\ntemp_avg_active['active_buckets'] = pd.qcut(temp_avg_active['active'],5, labels = False)\ntemp_avg_active = temp_avg_active.loc[:,['cfips','active_buckets']]\n\n# Final list of buckets\nactive_buckets = pd.concat((outlier_cfips_active, temp_avg_active))\nactive_buckets = active_buckets.reset_index(drop=\"index\")\n\nactive_buckets","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Assign the average micro business density buckets to data\neda_train_data = pd.merge(left = eda_train_data, right = active_buckets,on = 'cfips', how = 'left')\n\n# Trying to see how data ends up\n# Quick observations\n# Outlier buckets seem to have counties that started small (but had relatively higher density to start off with), but have high density over years\n# These counties may already have decent population\n\ntemp = eda_train_data.groupby(('active_buckets'), as_index = False).agg(min_density = (\"active\",np.min),max_density = (\"active\",np.max))\ntemp","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Quick scatter plot to visualise the outliers\n# sns.lineplot(data = temp, x = \"first_day_of_month\", y = \"active\", hue = \"active_buckets\")\ntemp = eda_train_data.loc[eda_train_data['active_buckets'] != \"outlier\",:]\ng = sns.FacetGrid(data = temp, col = \"active_buckets\", col_wrap = 3)\ng.map(sns.lineplot, \"first_day_of_month\", 'active')\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## ","metadata":{}},{"cell_type":"markdown","source":"## Census Data\n\nWe will plot census data and see if we can see a simple way to forecast it\n\nThe direct pairplot takes too long to process. We will try and shrink the data a bit by grouping it by states\n","metadata":{}},{"cell_type":"code","source":"# Updating the dataset\n\neda_census_data = census_data.copy()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"eda_census_data.columns","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The direct pairplot takes too long to process. We will try and shrink the data a bit by grouping it by states\n\ncol_set_2017 = ['cfips', 'pct_bb_2017', 'pct_college_2017', 'pct_foreign_born_2017', 'pct_it_workers_2017', 'median_hh_inc_2017']\ncol_set_2018 = ['cfips', 'pct_bb_2018', 'pct_college_2018', 'pct_foreign_born_2018', 'pct_it_workers_2018', 'median_hh_inc_2018']\ncol_set_2019 = ['cfips', 'pct_bb_2019', 'pct_college_2019', 'pct_foreign_born_2019', 'pct_it_workers_2019', 'median_hh_inc_2019']\ncol_set_2020 = ['cfips', 'pct_bb_2020', 'pct_college_2020', 'pct_foreign_born_2020', 'pct_it_workers_2020', 'median_hh_inc_2020']\ncol_set_2021 = ['cfips', 'pct_bb_2021', 'pct_college_2021', 'pct_foreign_born_2021', 'pct_it_workers_2021', 'median_hh_inc_2021']\n\n\nsns.pairplot(data = census_data.loc[:,col_set_2021])\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notes:\n\nPCT BB is coorelated with past year. Easy to forecast with growth variable\n\nPCR college may be coorelation to median hh income and pct bb\nPCT BB and PCT income may have some degree of coorelation\n\n","metadata":{}},{"cell_type":"code","source":"# thin_census_data\ncensus_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notes for later\n\n1. No 2022 census data. May need to project to get estimates","metadata":{}},{"cell_type":"markdown","source":"# RE:Pre-Processing: After validation\n\nBased on observations made, \n1. Census data lacks information for 2022, and needs to be generated/fetched from government website\n2. Training data is not lacking any essential information\n\nSteps:\n1. Project/impute the 2022 census data into the dataset\n2. Join the training set and the census data","metadata":{}},{"cell_type":"markdown","source":"#### Projecting census data\n\nWe can potentially use a slew of techniques here","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Merge the inputs provided\n\nWe will join the 2 datasets together on \"cfips\" and \"year\" columns.","metadata":{}},{"cell_type":"code","source":"train_base","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = train_base.merge(thin_census_data, on = [\"cfips\",\"year\"], how = 'left')\n\n# Using merge statament to avoid juggling index of dataframes\n# We will validate the row count, which should not increase if join is correct\ntrain_data = pd.merge(left = train_base,right = thin_census_data,how = 'left',on = ['cfips','year'])\ntrain_data","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}