{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# IMPORTANT\n\n## This work is continued in another notebook due to time required to preprocess text.\n\n## The final clean dataset with spell corrected and lemmatized input are saved as feather/parquet file and reimported in another book\n\n## Read for reference on using the file - https://arrow.apache.org/docs/python/parquet.html","metadata":{}},{"cell_type":"raw","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"# Install new libs","metadata":{}},{"cell_type":"code","source":"!pip install pyspellchecker","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:51:10.211241Z","iopub.execute_input":"2023-02-19T02:51:10.211605Z","iopub.status.idle":"2023-02-19T02:51:21.229905Z","shell.execute_reply.started":"2023-02-19T02:51:10.211582Z","shell.execute_reply":"2023-02-19T02:51:21.228437Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"Collecting pyspellchecker\n  Downloading pyspellchecker-0.7.1-py3-none-any.whl (2.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: pyspellchecker\nSuccessfully installed pyspellchecker-0.7.1\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# Import libraries","metadata":{}},{"cell_type":"code","source":"# Set random seed\n\nSEED = 1234509876\n\n# Importing basic libraries\nfrom zipfile import ZipFile\nimport os, sys\nimport re\nimport gc\nimport time\nimport datetime\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport json \nfrom string import punctuation\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\n%matplotlib inline\n\n# Import other libs\nimport emoji\nfrom spellchecker import SpellChecker\n\n# Import NLTK\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\n# # Preprocessing\n# from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n# from sklearn.model_selection import train_test_split, cross_val_score\n# from sklearn.preprocessing import StandardScaler\n\n# from sklearn.decomposition import PCA\n# from sklearn.metrics import f1_score\n\nfrom wordcloud import WordCloud\n# Import models\n\n# import lightgbm as lgb\n# from sklearn.cluster import KMeans\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n# from sklearn.naive_bayes import GaussianNB\n\n# Model selection\n# from sklearn.model_selection import RandomizedSearchCV\n\n# Others\n\nfrom tqdm import tqdm_notebook #Loads progressbars for various loops\n\nimport warnings\n\nwarnings.filterwarnings('ignore')\n\n#####################\n#Useful pandas settings\n\npd.set_option('display.max_rows', 400)\npd.set_option('display.max_columns', 160)\npd.set_option('display.max_colwidth', 40)\nwarnings.filterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:52:34.559938Z","iopub.execute_input":"2023-02-19T02:52:34.561185Z","iopub.status.idle":"2023-02-19T02:52:34.573412Z","shell.execute_reply.started":"2023-02-19T02:52:34.561133Z","shell.execute_reply":"2023-02-19T02:52:34.572237Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Custom Functions\n\nCollecting all functions here for easy reference and update","metadata":{}},{"cell_type":"code","source":"################################################################################################\n# Downcasting function for pandas dataframes\n\ndef downcast_dtypes(df):\n    '''\n    Changes column types in the dataframe:             \n      `float64` type to lowest possible float without data loss\n      `int64`   type to lowest possible int wihtout data loss\n    '''\n\n    # Select columns to downcast\n    float_cols = [col for col in df if df[col].dtype == \"float64\"]\n    int_cols =   [col for col in df if df[col].dtype == \"int64\"]\n\n    # Downcast columns using to numeric function\n    df[float_cols] = df[float_cols].apply(pd.to_numeric, downcast='float')\n    df[int_cols] = df[int_cols].apply(pd.to_numeric, downcast='integer')\n\n    # remove variables from memory to avoid issues\n\n    del float_cols\n    del int_cols\n\n    return df\n\n################################################################################################\n# Check duplication at given level of dataframe\n\ndef check_dups(df, cols):\n\n    orig_count_rows = df.shape[0]\n\n    temp = df.groupby(cols).size().reset_index(name = 'counts')\n\n    dedup_count_rows = temp.shape[0]\n\n    if orig_count_rows == dedup_count_rows:\n        print(\"No duplicates. Dataframe is unique at given level\")\n        print(\"# of unique entries: n=\",orig_count_rows)\n    else:\n        print(\"Duplicates found. Dataframe is not unique at given level\")\n        print(\"# of entries in original dataset: n=\", orig_count_rows)\n        print(\"# of unique entries expected in deduped dataset: n=\", dedup_count_rows)\n        print(\"# of addational entries: n=\", orig_count_rows - dedup_count_rows)\n\n    del orig_count_rows, temp, dedup_count_rows\n\n#####################################################################################\n# Plotting classification features\ndef fancy_plot(df):\n    column_names = list(df.columns.values)\n    frauds = df[df['Class'] == 1]\n    no_frauds = df[df['Class'] == 0]\n\n    plt.figure()\n    fig, ax = plt.subplots(8,4,figsize=(16,28))\n    i = 0\n    for feature in column_names:\n        i += 1\n        plt.subplot(8,4,i)\n        sns.kdeplot(frauds[feature])\n        sns.kdeplot(no_frauds[feature])\n        plt.xlabel(feature, fontsize=10)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='both', which='major', labelsize=12)\n    plt.show();\n\n####################################################################################\n\n########################################\n#Custom function to apply functions to dataframe with missing values\ndef impute_missing(df, func, target_col, new_col_name):\n    df.loc[~df[target_col].isnull(),new_col_name] = df.loc[~df[target_col].isnull(),target_col].apply(func)\n\n\n####################################################################################\n#text cleaning and stemming function. Modified to cater to text provided\n\ndef remove_links(raw):\n    # Extracts links from input text. Returns both text and links \n    link_expr = 'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*(),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+.'\n    \n    # Check if passed object is single string or series\n    if type(raw) == str:\n        no_link_raw = re.sub(link_expr,\"\",raw)\n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        no_link_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            no_link_raw.append(re.sub(link_expr, \"\", tweet))\n            \n    return no_link_raw\n    \ndef remove_hashtags(raw):\n    # Extracts links from input text. Returns both text and links\n    # Will remove all trailing hashtags\n    # Hashtags in middle of text will be replaced by a \"SOME_ENTITY\" constant text with hoope to retain context\n    \n    hashtag_expr = '#[A-Za-z0-9]+'\n    middle_tag_expr = '#[A-Za-z0-9]+^[#]'\n    # tags = re.findall(hashtag_expr,\"\",raw)\n    \n    if type(raw) == str:\n        no_tag_raw = re.sub(hashtag_expr,\"\",raw)\n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        no_tag_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            no_tag_raw.append(re.sub(hashtag_expr, \"\", tweet))\n            \n    return no_tag_raw\n    \ndef replace_mentions(raw):\n    # Replaces personal mentions with a common entity tag.\n    # As we cannot build context on specific persons, we will tag it as entity and let our model identify language patterns\n    mention_expr = '@[A-Za-z0-9]+'\n    # tags = re.findall(hashtag_expr,\"\",raw)\n    \n    if type(raw) == str:\n        no_mention_raw = re.sub(mention_expr,\" SOME_ENTITY \",raw) # Space to avoid potential merging with other words. \n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        no_tag_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            no_tag_raw.append(re.sub(mention_expr,\" SOME_ENTITY \",tweet)) # Space to avoid potential merging with other words.\n            \n    return no_tag_raw\n\ndef trim_extra_space(raw):\n    space_expr = '\\s+'\n    # tags = re.findall(hashtag_expr,\"\",raw)\n    \n    if type(raw) == str:\n        clean_raw = re.sub(space_expr,\" \",raw)\n        clean_raw = clean_raw.strip(\" \") # Remove end trails\n        # links = re.findall(link_expr,\"\",raw)\n    else:\n        clean_raw = list()\n        # Disabled link extraction for now\n        # links = list() \n        \n        for tweet in raw:\n            temp = re.sub(space_expr,\" \",tweet)\n            clean_raw.append(temp.strip(\" \"))\n            \n    return clean_raw\n\ndef clean_text(raw):\n    # Combine all cleaning work\n    cleaned_text = remove_links(raw)\n    cleaned_text = remove_hashtags(cleaned_text)\n    cleaned_text = replace_mentions(cleaned_text)\n    cleaned_text = trim_extra_space(cleaned_text)    \n\n    return cleaned_text\n\ndef simple_emoji_list(text):\n    # Modification to emoji list function.\n    # Removes the start and end character poiitns, and just retains actual emojis for easy parsing\n    emojis = emoji.emoji_list(text)\n    clean_list = list()\n    if len(emojis)>0:\n        for each in emojis:\n            clean_list.append(emoji.demojize(each['emoji']))\n    \n    return clean_list\n  \n\n# def token_converter():\n    # Convert text to tokens\n    \n#     tokens = nltk.word_tokenize(temp)\n    \n#     alph_num_tokens = [word for word in tokens if word.isalnum()]\n#     non_alph_num_tokens = [word for word in tokens if not word.isalnum()]\n\n#     non_alph_num_tokens = [word.split('-') for word in non_alph_num_tokens]\n#     non_alph_num_tokens = nltk.flatten(non_alph_num_tokens)\n#     non_alph_num_tokens = [word.split('.') for word in non_alph_num_tokens]\n#     non_alph_num_tokens = nltk.flatten(non_alph_num_tokens)\n\n#     alph_num_tokens.extend(non_alph_num_tokens)\n\n#     tokens = nltk.flatten(alph_num_tokens)\n\n#     tokens = [porter.stem(word.lower()) for word in tokens]\n#     tokens = [word for word in tokens if word not in stopwords_en]\n#     tokens = [word for word in tokens if word.isalnum()]\n\n#     return tokens\n\n    #####################################################\n# Generate word clouds\n\ndef generate_wordclouds(X, in_X_tfidf, k, in_word_positions):\n    # compute the total tfidf for each term in the cluster\n    in_tfidf = in_X_tfidf[in_y_pred == in_cluster_id]\n    # numpy.matrix\n    tfidf_sum = np.sum(in_tfidf, axis=0)\n    # numpy.array of shape (1, X.shape[1])\n    tfidf_sum = np.asarray(tfidf_sum).reshape(-1)\n    top_indices = tfidf_sum.argsort()[-top_count:]\n    term_weights = {in_word_positions[in_idx]: tfidf_sum[in_idx] for in_idx in top_indices}\n    wc = WordCloud(width=1200, height=800, background_color=\"white\")\n    wordcloud = wc.generate_from_frequencies(term_weights)\n    fig, ax = plt.subplots(figsize=(10, 6), dpi=100)\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    fig.suptitle(f\"Cluster {in_cluster_id}\")\n    plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:51:22.137052Z","iopub.execute_input":"2023-02-19T02:51:22.137341Z","iopub.status.idle":"2023-02-19T02:51:22.163442Z","shell.execute_reply.started":"2023-02-19T02:51:22.137312Z","shell.execute_reply":"2023-02-19T02:51:22.161876Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# Import dataset\n\nDataset has been uploaded to kaggle repo for easy access","metadata":{}},{"cell_type":"code","source":"raw_data = pd.read_csv('/kaggle/input/sarcasm/all_twitter_sarcasam.csv')\n\n# Remove extra columns from the data\nkeep_cols = ['id','text']\nraw_data = raw_data.loc[:,keep_cols]\n\n# Convert tweets to lowcase before proceeding with anything\nraw_data['text'] = raw_data['text'].str.lower()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.951826Z","iopub.status.idle":"2023-02-19T02:49:44.952212Z","shell.execute_reply.started":"2023-02-19T02:49:44.952058Z","shell.execute_reply":"2023-02-19T02:49:44.952073Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Label data","metadata":{}},{"cell_type":"code","source":"temp = raw_data.loc[:,'text']\nlabels = list()\nsarcasm_tags = set((\"#sarcasm\",\"#sarcastic\"))\n\nfor tweet in temp:\n    lowcase_tweet = tweet.lower()\n    hashtags = set(re.findall('#[A-Za-z0-9]+',lowcase_tweet))\n    if any(hashtags & sarcasm_tags):\n        labels.append(\"sarcasm\")\n    else:\n        labels.append(\"non-sarcasm\")\n        \nraw_data['labels'] = labels\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.953723Z","iopub.status.idle":"2023-02-19T02:49:44.954140Z","shell.execute_reply.started":"2023-02-19T02:49:44.953930Z","shell.execute_reply":"2023-02-19T02:49:44.953950Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Cleanup\n\n## Steps done:\n1. Cleanup text:\n    1. Lowercase text\n    1. Remove hashtags\n    1. Replace mentions with common tag \"SOME_ENTITY\"\n    1. remove links (usually adverts have them)\n1. Extract list of emojis for easy reference.\n    1. Replace emojis with their text equivalents\n    1. Remove the skin tone color information as that may not help with context\n1. De-contract short form words (e.g. it's =>> it is)\n1. TODO - Spell checking (not sure if it will help somehow.. Need to test later)\n1. Lemmatization - Cannot use for BERT. Maybe we use normal word embeddings for first latyer, and BERT for 2nd..NOt sure","metadata":{}},{"cell_type":"code","source":"raw_data['clean_text'] = clean_text(raw_data['text'])\n\n# Remove duplicates. There are ton of ads that will be easy to remove after cleanup\n# Ads contain different hashtags so cant be deduped raw\n\nraw_data.drop_duplicates(subset = 'clean_text', inplace = True)\nraw_data = raw_data.reset_index().drop(columns = 'index')","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.956019Z","iopub.status.idle":"2023-02-19T02:49:44.956479Z","shell.execute_reply.started":"2023-02-19T02:49:44.956230Z","shell.execute_reply":"2023-02-19T02:49:44.956270Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Extract emojis and replace with text\nWe will dedup and check if we need to standardize emojis and remove tone info","metadata":{}},{"cell_type":"code","source":"# Extract emojis for analysis\nraw_data['emoji_list'] = raw_data['clean_text'].apply(simple_emoji_list)\n# Replace emojis with text versions\nraw_data['clean_text'] = raw_data['clean_text'].apply(emoji.demojize)\n# Remove tone info to sandardize emojis\nraw_data['clean_text'] = raw_data['clean_text'].apply(lambda text: re.sub('_[A-Za-z -]+_skin_tone:', \":\", text))\nraw_data['emoji_list'] = raw_data['emoji_list'].apply(lambda text: [re.sub('_[A-Za-z -]+_skin_tone:', \":\", x) for x in text])\n\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.957925Z","iopub.status.idle":"2023-02-19T02:49:44.958359Z","shell.execute_reply.started":"2023-02-19T02:49:44.958123Z","shell.execute_reply":"2023-02-19T02:49:44.958144Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Convert shortforms to longforms\n\nSolution is courtsey of https://stackoverflow.com/questions/43018030/replace-apostrophe-short-words-in-python","metadata":{}},{"cell_type":"code","source":"def decontracted(phrase):\n    # Including all sorts of inverted commas to ensure text gets converted without issues with different posts using different keyboards\n    \n    # Specific exceptions to avoid mis spelled words\n    phrase = re.sub(r\"won[\\'\\`\\’]t\", \"will not\", phrase)\n    phrase = re.sub(r\"can[\\'\\`\\’]t\", \"can not\", phrase)\n\n    # general concatenations\n    phrase = re.sub(r\"n[\\'\\`\\’]t\", \" not\", phrase)\n    phrase = re.sub(r\"[\\'\\`\\’]re\", \" are\", phrase)\n    phrase = re.sub(r\"[\\'\\`\\’]s\", \" is\", phrase)\n    phrase = re.sub(r\"[\\'\\`\\’]d\", \" would\", phrase)\n    phrase = re.sub(r\"[\\'\\`\\’]ll\", \" will\", phrase)\n    phrase = re.sub(r\"[\\'\\`\\’]t\", \" not\", phrase)\n    phrase = re.sub(r\"[\\'\\`\\’]ve\", \" have\", phrase)\n    phrase = re.sub(r\"[\\'\\`\\’]m\", \" am\", phrase)\n    return phrase\n\nprint(raw_data.loc[11,'clean_text'])\nraw_data['clean_text'] = raw_data['clean_text'].apply(decontracted)\n","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.960760Z","iopub.status.idle":"2023-02-19T02:49:44.961175Z","shell.execute_reply.started":"2023-02-19T02:49:44.961004Z","shell.execute_reply":"2023-02-19T02:49:44.961021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Spell checking - RUNS SLOW. Expect 1-2 seconds per tweet. Ignore until final dataset\n\nToo time consuming. May need to run it once and store final data","metadata":{}},{"cell_type":"code","source":"spell_checker = SpellChecker(distance=0) # TO reduce processing time and pick 1 closest word automatically.\n\n# Lambda function to return original word if spell check does not find anything\ncustom_spell_check = lambda word: word if spell_checker.correction(word) == None else spell_checker.correction(word)\n\n# raw_data['clean_text'] = raw_data['clean_text'].apply(lambda text: )\nraw_data['clean_text'] = raw_data['clean_text'].map(lambda x:\" \".join(custom_spell_check(word) for word in x.split(\" \")))\n\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.963241Z","iopub.status.idle":"2023-02-19T02:49:44.963781Z","shell.execute_reply.started":"2023-02-19T02:49:44.963602Z","shell.execute_reply":"2023-02-19T02:49:44.963619Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Saving the dataset as it takes too long now\nimport pyarrow as pa\nimport pyarrow.parquet as pq\n\nsave_raw_data = pa.Table.from_pandas(raw_data)\npq.write_table(save_raw_data, '/kaggle/working/clean_raw_data.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.964930Z","iopub.status.idle":"2023-02-19T02:49:44.965242Z","shell.execute_reply.started":"2023-02-19T02:49:44.965096Z","shell.execute_reply":"2023-02-19T02:49:44.965111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Lemmatization\n\nBERT should not like stopword removal and lemmatization due to how it works. \nWe will make a new column to store lemmatized + stopword removed text and see if performance changes","metadata":{}},{"cell_type":"code","source":"# # Import code for resume\n# temp = pq.read_table('/kaggle/input/sarcasm/clean_raw_data.parquet')\n\n# raw_data = temp.to_pandas()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:53:46.095740Z","iopub.execute_input":"2023-02-19T02:53:46.096098Z","iopub.status.idle":"2023-02-19T02:53:46.128928Z","shell.execute_reply.started":"2023-02-19T02:53:46.096073Z","shell.execute_reply":"2023-02-19T02:53:46.127946Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"temp = raw_data.loc[:,['clean_text']]\n\nlemmatizer = WordNetLemmatizer()\nstopword_list = stopwords.words('english')\n\n# Can possibly integrate POS here it seems\nraw_data['clean_text_lem'] = raw_data['clean_text'].apply(lambda tweet: \" \".join(lemmatizer.lemmatize(word) for word in tweet.split(\" \")))\n# Remove stopwords\nraw_data['clean_text_lem_stop'] = raw_data['clean_text_lem'].apply(lambda tweet: \" \".join(word for word in tweet.split(\" \") if word not in stopword_list))\n\nraw_data.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T03:27:18.258508Z","iopub.execute_input":"2023-02-19T03:27:18.258826Z","iopub.status.idle":"2023-02-19T03:27:18.407308Z","shell.execute_reply.started":"2023-02-19T03:27:18.258803Z","shell.execute_reply":"2023-02-19T03:27:18.406120Z"},"trusted":true},"execution_count":18,"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"                    id                                     text   labels  \\\n0  1623471399825293312  @annielayer @repmtg i'm sure the ame...  sarcasm   \n1  1623470696125923329  get my art printed on awesome produc...  sarcasm   \n2  1623467236982947842  trudeau? anyone? #tuckercarlson #unh...  sarcasm   \n3  1623465163792711681  nuh uh. #joebiden told me everything...  sarcasm   \n4  1623465117869395968  😂 she gave #biden the #chinaballoon ...  sarcasm   \n\n                                clean_text  \\\n0  SOME_ENTITY SOME_ENTITY i am sure th...   \n1  get my art printed on awesome produc...   \n2                          trudeau? anyone   \n3  nuh uh told me everything was fine.:...   \n4  :face_with_tears_of_joy: she gave th...   \n\n                                emoji_list  \\\n0                                       []   \n1                                       []   \n2                                       []   \n3                      [:woman_shrugging:]   \n4  [:face_with_tears_of_joy:, :rolling_...   \n\n                            clean_text_lem  \\\n0  SOME_ENTITY SOME_ENTITY i am sure th...   \n1  get my art printed on awesome produc...   \n2                          trudeau? anyone   \n3  nuh uh told me everything wa fine.:w...   \n4  :face_with_tears_of_joy: she gave th...   \n\n                       clean_text_lem_stop  \n0  SOME_ENTITY SOME_ENTITY sure america...  \n1  get art printed awesome product redo...  \n2                          trudeau? anyone  \n3  nuh uh told everything wa fine.:woma...  \n4  :face_with_tears_of_joy: gave :rolli...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>text</th>\n      <th>labels</th>\n      <th>clean_text</th>\n      <th>emoji_list</th>\n      <th>clean_text_lem</th>\n      <th>clean_text_lem_stop</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1623471399825293312</td>\n      <td>@annielayer @repmtg i'm sure the ame...</td>\n      <td>sarcasm</td>\n      <td>SOME_ENTITY SOME_ENTITY i am sure th...</td>\n      <td>[]</td>\n      <td>SOME_ENTITY SOME_ENTITY i am sure th...</td>\n      <td>SOME_ENTITY SOME_ENTITY sure america...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1623470696125923329</td>\n      <td>get my art printed on awesome produc...</td>\n      <td>sarcasm</td>\n      <td>get my art printed on awesome produc...</td>\n      <td>[]</td>\n      <td>get my art printed on awesome produc...</td>\n      <td>get art printed awesome product redo...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1623467236982947842</td>\n      <td>trudeau? anyone? #tuckercarlson #unh...</td>\n      <td>sarcasm</td>\n      <td>trudeau? anyone</td>\n      <td>[]</td>\n      <td>trudeau? anyone</td>\n      <td>trudeau? anyone</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1623465163792711681</td>\n      <td>nuh uh. #joebiden told me everything...</td>\n      <td>sarcasm</td>\n      <td>nuh uh told me everything was fine.:...</td>\n      <td>[:woman_shrugging:]</td>\n      <td>nuh uh told me everything wa fine.:w...</td>\n      <td>nuh uh told everything wa fine.:woma...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1623465117869395968</td>\n      <td>😂 she gave #biden the #chinaballoon ...</td>\n      <td>sarcasm</td>\n      <td>:face_with_tears_of_joy: she gave th...</td>\n      <td>[:face_with_tears_of_joy:, :rolling_...</td>\n      <td>:face_with_tears_of_joy: she gave th...</td>\n      <td>:face_with_tears_of_joy: gave :rolli...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Save output for easy use::\n\n\nsave_raw_data = pa.Table.from_pandas(raw_data)\npq.write_table(save_raw_data, '/kaggle/working/clean_raw_data.parquet')","metadata":{"execution":{"iopub.status.busy":"2023-02-19T03:28:06.001515Z","iopub.execute_input":"2023-02-19T03:28:06.001874Z","iopub.status.idle":"2023-02-19T03:28:06.023235Z","shell.execute_reply.started":"2023-02-19T03:28:06.001850Z","shell.execute_reply":"2023-02-19T03:28:06.022558Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"# Quick EDA\n\nCheck simple word density based on the labelled tweets","metadata":{}},{"cell_type":"code","source":"sarcasm_tweets = raw_data.loc[raw_data['labels'] == 'sarcasm','clean_text']\nsarcasm_all_words = ' '.join(sarcasm_tweets)\n\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(width=800, height=800, background_color='white', max_words=100, colormap='viridis', contour_width=3, contour_color='black')\n\n# Generate the word cloud\nwordcloud.generate(sarcasm_all_words)\nplt.figure(figsize=(8,8), facecolor=None)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.tight_layout(pad=0)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.970379Z","iopub.status.idle":"2023-02-19T02:49:44.970672Z","shell.execute_reply.started":"2023-02-19T02:49:44.970526Z","shell.execute_reply":"2023-02-19T02:49:44.970543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Simple table with frequency to check if there has been issue with prepriocessing\ntqdm_notebook.pandas(desc = \"Spell check progress\")\nall_word_list = pd.DataFrame(data = (\" \".join(raw_data['clean_text']).split(\" \")), columns = [\"words\"])\nall_word_list['count'] = 1\n\nword_count = all_word_list.groupby(['words'],as_index = False).count()\nword_count.sort_values(by = 'count', ascending=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.971825Z","iopub.status.idle":"2023-02-19T02:49:44.972125Z","shell.execute_reply.started":"2023-02-19T02:49:44.971974Z","shell.execute_reply":"2023-02-19T02:49:44.971988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature Generation","metadata":{}},{"cell_type":"code","source":"plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-02-19T02:49:44.973123Z","iopub.status.idle":"2023-02-19T02:49:44.973467Z","shell.execute_reply.started":"2023-02-19T02:49:44.973328Z","shell.execute_reply":"2023-02-19T02:49:44.973342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Notes::\n\n1. Maybe number of hashtags as feature. ","metadata":{}}]}